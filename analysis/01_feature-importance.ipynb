{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ac7adc-1eb1-442a-9d3e-baae198ab6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cfeeea-eeb7-46b4-bc62-6d50d3aaf8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank = pd.read_csv(\"../data/bank-full.csv\", delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b016c-4a35-4eae-b9b5-b9b0cc3fb143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df_bank, test_size=0.25, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c4057-77cd-4980-ab8a-44661213a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b57a2c7-f142-4ecd-a5dc-4618c0f45cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e310703-fc87-450f-a728-0597084147ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29405d93-4274-4173-84cb-c7cf47d8478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"y\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d652ecdc-0613-4c64-9e9d-ecb60e76a394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributions of categorical and numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada49f7c-58dd-456a-b86a-bdd25c6c08e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = list(train_df.drop(columns=[\"y\"]).select_dtypes(include=[\"object\"]).columns)\n",
    "numerical_cols = list(train_df.select_dtypes(include=[\"int64\"]).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0421d0a-db05-4ab9-b24f-05cff7d333ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "alt.data_transformers.enable(\"vegafusion\")\n",
    "\n",
    "alt.Chart(train_df).mark_bar().encode(\n",
    "    x=\"count()\",\n",
    "    y=alt.Y(alt.repeat()).type(\"nominal\")\n",
    ").repeat(\n",
    "    categorical_cols, columns=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ae9e24-387a-445f-aa43-475fa1c485af",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(train_df).mark_bar().encode(\n",
    "    x=alt.X(alt.repeat()).type(\"quantitative\").bin(maxbins=40),\n",
    "    y=\"count()\"\n",
    ").repeat(\n",
    "    numerical_cols, columns=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edca1d09-71fb-47d8-8876-c1c21bfa2510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations between numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336175e8-115e-4a20-90cf-f63d71d4084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure linear relationship\n",
    "person_corr_df = train_df[numerical_cols].corr(\"pearson\").unstack().reset_index()\n",
    "person_corr_df.columns = [\"num_variable_0\", \"num_variable_1\", \"correlation\"]\n",
    "\n",
    "corr_heatmap = alt.Chart(person_corr_df).mark_rect().encode(\n",
    "    x=alt.X(\"num_variable_0\").title(\"Numerical Variable\"),\n",
    "    y=alt.Y(\"num_variable_1\").title(\"Numerical Variable\"),\n",
    "    color=\"correlation:Q\"\n",
    ").properties(\n",
    "    width=250,\n",
    "    height=250\n",
    ")\n",
    "\n",
    "text = alt.Chart(person_corr_df, title=\"Pearson Correlation\").mark_text().encode(\n",
    "    x=alt.X(\"num_variable_0\").title(\"Numerical Variable\"),\n",
    "    y=alt.Y(\"num_variable_1\").title(\"Numerical Variable\"),\n",
    "    text=alt.Text(\"correlation:Q\", format=\".2f\")\n",
    ")\n",
    "\n",
    "corr_heatmap + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47b952e-e67f-4481-9234-82f2b9cd0b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure monotonic (incl. non-linear) relationship\n",
    "# FIXME: does it matter? If we apply linear models, we're more afraid of collinearity, i.e. linear relationship\n",
    "spearman_corr_df = train_df[numerical_cols].corr(\"spearman\").unstack().reset_index()\n",
    "spearman_corr_df.columns = [\"num_variable_0\", \"num_variable_1\", \"correlation\"]\n",
    "\n",
    "corr_heatmap = alt.Chart(spearman_corr_df, title=\"Spearman Correlation\").mark_rect().encode(\n",
    "    x=alt.X(\"num_variable_0\").title(\"Numerical Variable\"),\n",
    "    y=alt.Y(\"num_variable_1\").title(\"Numerical Variable\"),\n",
    "    color=\"correlation:Q\"\n",
    ").properties(\n",
    "    width=250,\n",
    "    height=250\n",
    ")\n",
    "\n",
    "text = alt.Chart(spearman_corr_df).mark_text().encode(\n",
    "    x=alt.X(\"num_variable_0\").title(\"Numerical Variable\"),\n",
    "    y=alt.Y(\"num_variable_1\").title(\"Numerical Variable\"),\n",
    "    text=alt.Text(\"correlation:Q\", format=\".2f\")\n",
    ")\n",
    "\n",
    "corr_heatmap + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e078ec77-0a66-47cd-9bfd-5c514ee74d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for correlation between pdays and previous\n",
    "# FIXME: there is no linear relationship here\n",
    "pdays_prev = alt.Chart(train_df, title=\"pdays vs previous\").mark_point().encode(\n",
    "    x=\"pdays\",\n",
    "    y=\"previous\"\n",
    ")\n",
    "\n",
    "pdays_prev_clamped = alt.Chart(train_df, title=\"pdays vs previous (previous <= 50)\").mark_point().encode(\n",
    "    x=\"pdays\",\n",
    "    y=alt.Y(\"previous\").scale(domain=(0, 50), clamp=True)\n",
    ")\n",
    "\n",
    "pdays_prev | pdays_prev_clamped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8e0439-b7ac-438d-b922-9af48386a661",
   "metadata": {},
   "source": [
    "### Summary and Recommendations from EDA\n",
    "- Generally, bar charts were created for categorical variables, and histograms for numerical variables to show illustration. Correlation heatmaps based on two different metrics were generated to investigate the relationships between numerical variables. A scatter plot specifically for `pdays` vs `previous` was created.\n",
    "- Judging from the proportion of each class in the target, the dataset is unbalanced\n",
    "- `job`, `education`, `contact` and `poutcome` contain unknown values. We do not have enough information on the dataset to impute these values properly. Note that these values are not null values, but strings called \"unknown\". **# FIXME: it seems only `contact` and `poutcome` have \"unknown\"**\n",
    "- Out of the columns mentioned that contain unknown values, `contact` and `poutcome` need to be dropped since they contain too many unknown examples. We cannot just drop the unknowns from these columns since we would be dropping too many examples, especially considering the size of the data. **# FIXME: `poutcome` is the outcome of the previous marketing campaign. Most of the \"unknown\" is because `previous = 0`. If the person has never been contacted for marketing before, it makes sense to say \"unknown\" for this field as \"previous marketing\" doesn't exist. But for those who have been reached out before, this `poutcome` might be informative! On the other hand, I think `contact` could be kept, because not that many \"unknown\" in it actually. Also, our task is to find out the importance of features. There should be no farm to keep it, but I would expect the result to tell me that this `contact` is not important.**\n",
    "- `job` and `education` can be kept. We can just drop the unknowns from these features. **# FIXME: they might contain information too?**\n",
    "- The distributions of `pdays` and `previous` are heavily skewed. These variables are also correlated with 0.99 Spearman correlation score and 0.44 Pearson correlation score.\n",
    "- However, upon visual inspection with a scatter plot, `pdays` and `previous` do not seem to be too correlated to be an issue. We can keep them both as features.\n",
    "- Overall recommendations:\n",
    "   - Drop `contact` and `poutcome`\n",
    "   - Drop unknown values from `job` and `education`\n",
    "   - Ordinal encode `education`\n",
    "   - One-hot encode other categorical variables\n",
    "   - Standardize numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862830a9-7577-4f1c-b572-e0b27609a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df.loc[(train_df[\"job\"] != \"unknown\") & (train_df[\"education\"] != \"unknown\")]\n",
    "# test_df = test_df.loc[(test_df[\"job\"] != \"unknown\") & (test_df[\"education\"] != \"unknown\")]\n",
    "\n",
    "# X_train = train_df.drop(columns=[\"y\"])\n",
    "# y_train = train_df[\"y\"]\n",
    "# X_test = test_df.drop(columns=[\"y\"])\n",
    "# y_test = test_df[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d8961-a406-43f7-a1b4-84300f9cb0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.compose import make_column_transformer\n",
    "# from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "# # recommended transformations\n",
    "# categorical_feats = [\"job\", \"marital\", \"default\", \"housing\", \"loan\", \"month\"]\n",
    "# ordinal_feats = [\"education\"]\n",
    "# drop_feats = [\"contact\", \"poutcome\"]\n",
    "# numerical_feats = numerical_cols\n",
    "\n",
    "# education_levels = [\"primary\", \"secondary\", \"tertiary\"]\n",
    "\n",
    "# col_transformer = make_column_transformer((OneHotEncoder(sparse_output=False, drop=\"if_binary\"), categorical_feats),\n",
    "#                                           (OrdinalEncoder(categories=[education_levels], dtype=int), ordinal_feats),\n",
    "#                                           (\"drop\", drop_feats),\n",
    "#                                           (StandardScaler(), numerical_feats))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e132497-3e21-4e98-a8fc-96350ce7fa5d",
   "metadata": {},
   "source": [
    "### notes\n",
    " - In this study, we wanna understand which factors would affect the most for clients' subscription to the term deposit\n",
    " - positive label: `y = \"yes\"`; negative label: `y = \"no\"`\n",
    " - We don't wanna miss any potential clients. Therefore, we wanna lower as much as possible the Type I error / false positive, i.e. clients being identified as subscribed to our term deposit but actually they didn't.\n",
    " - We will select a model that gives us a robust **precision**, so that the model could better explain clients' motivation to the subscription. We would put more trust to the feature importance recommended by the model.\n",
    " - In below, I didn't demonstrate different `max_depth` trials for the `DecisionTreeClassifier`, because our focus is on feature importance rather than the prediction, and I found that the recommendation from the Decision Tree is very similar to the Logistic Regression. Honestly, I think we could actually consider dropping the Decision Tree and just use Logistic Regression for the feature importance recommendation. I'm open to that (coz I also think that keeping the Decision Tree there would make the analysis looks more fancy which seems a good thing too?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61cb62e-9de9-4fe8-8cd0-e7298eaf6dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_df.drop(columns=[\"y\"]), train_df[\"y\"]\n",
    "X_test, y_test = test_df.drop(columns=[\"y\"]), test_df[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc14744-068a-4981-acbc-2ba823d9be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "categorical_feats = [\"job\", \"marital\", \"default\", \"housing\", \"loan\", \"contact\", \"day\", \"month\", \"poutcome\"]\n",
    "ordinal_feats = [\"education\"]\n",
    "numeric_feats = [\"age\", \"balance\", \"duration\", \"campaign\", \"previous\", \"pdays\"]\n",
    "#drop_feats = [\"month\", \"day\"]\n",
    "\n",
    "education_levels = [\"unknown\", \"primary\", \"secondary\", \"tertiary\"]\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (OneHotEncoder(sparse_output=False, drop=\"if_binary\"), categorical_feats),\n",
    "    (OrdinalEncoder(categories=[education_levels], dtype=int), ordinal_feats),\n",
    "    (StandardScaler(), numeric_feats),\n",
    "#    (\"drop\", drop_feats)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3addaedc-7da2-400a-b633-c8d340ac8bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_pipes = {\n",
    "    \"Baseline\": DummyClassifier(strategy=\"most_frequent\", random_state=522),\n",
    "    \"DecisionTree\": make_pipeline(preprocessor, DecisionTreeClassifier(max_depth=5, random_state=522)),\n",
    "    \"LogisticRegression\": make_pipeline(preprocessor, LogisticRegression(max_iter=2000, random_state=522)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b3ce3a-fa4e-426f-8bcc-551e49d51114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, precision_score\n",
    "\n",
    "mod_precision_score = make_scorer(precision_score, zero_division=0)\n",
    "\n",
    "classification_metrics = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"precision\": mod_precision_score,\n",
    "    \"recall\": \"recall\", \n",
    "}\n",
    "cross_val_results = {}\n",
    "\n",
    "for name, pipe in model_pipes.items():\n",
    "    cross_val_results[name] = pd.DataFrame(\n",
    "        cross_validate(\n",
    "            pipe, \n",
    "            X_train, \n",
    "            y_train==\"yes\", \n",
    "            cv=5,\n",
    "            return_train_score=True, \n",
    "            scoring=classification_metrics)\n",
    "    ).agg(['mean', 'std']).round(3).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0092c04f-8a79-4877-b1cc-5ce117047e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    cross_val_results,\n",
    "    axis='columns'\n",
    ").xs(\n",
    "    'mean',\n",
    "    axis='columns',\n",
    "    level=1\n",
    ").style.format(\n",
    "    precision=2\n",
    ").background_gradient(\n",
    "    axis=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf82c85e-e4f7-49cc-bd56-de385b1a023b",
   "metadata": {},
   "source": [
    "### note:\n",
    " - `LogisticRegression` has slightly better test precision than the `DecisionTreeClassifier`, but the `LogisticRegression` has a smaller gap between train scores and test scores, so `LogisticRegression` is more likely to generalize.\n",
    " - Limitation: there is still room of improvements on the precision\n",
    "   - In the future, if we ever need to make prediction of the subscription, we could increase the threshold to yield a better precision. So, I guess it's not so much worry here.\n",
    "   - If we really keen on improving the precision on the model level, I could see RandomForest is a choice, but not everyone of us know RandomForest yet, haha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfd9162-25fa-40e3-acbf-669d1ebd09c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipe = model_pipes['LogisticRegression']\n",
    "\n",
    "lr_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b54d186-e604-443f-ad37-5981d8aa9292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, lr_pipe.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b186e938-9219-42ec-932a-15f69277c9d3",
   "metadata": {},
   "source": [
    "### note\n",
    " - The precision test score is similar to the validation score as well as the train score. Therefore, we would believe that the feature importance conclusion drawn from this model is generalizable to the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7857a682-ab5a-4bce-ace0-07f226e097e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = lr_pipe.named_steps['columntransformer'].named_transformers_['onehotencoder'].get_feature_names_out().tolist()\n",
    "ordinal_cols = lr_pipe.named_steps['columntransformer'].named_transformers_['ordinalencoder'].get_feature_names_out().tolist()\n",
    "numeric_cols = lr_pipe.named_steps['columntransformer'].named_transformers_['standardscaler'].get_feature_names_out().tolist()\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': categorical_cols + ordinal_cols + numeric_cols, \n",
    "    'coef': lr_pipe.named_steps['logisticregression'].coef_[0].tolist()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5495024-da78-444a-ac94-6e98125fbf4d",
   "metadata": {},
   "source": [
    "### note\n",
    " - According to the below heatmap, `poutcome`, `month` and `duration` are the top 3 features that are highly related to whether a client would subscribe to the term deposit or not.\n",
    "   - If `poutcome == \"success\"`, clients were already experiencing the good services by the bank, so they're more willing to subscribe new products.\n",
    "   - If `month == \"mar\"`, I'm not sure why clients tend to accept the marketing and subscribe the term deposit in March, this pattern also exists in the test set, so it's not likely an over-fitting. I thought it might be related to the financial/tax period/bonus release time in Portugal, but it seems the finance year-end in Portugal is December (it could be March in some countries e.g. China). So I still can't make sense of it yet.\n",
    "   - If `duration` is longer, that means the clients were more interested in the term deposit product and were more likely to stay on the call, the salesperson had time to do more pitching, so increase the chance of successful subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554b8792-d342-4ec7-89f1-e3c68758e4d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importance.sort_values('coef', ascending=False).style.format(\n",
    "    precision=3\n",
    ").background_gradient(\n",
    "    cmap=\"PiYG\",\n",
    "    vmin=-2,\n",
    "    vmax=2,\n",
    "    axis=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07acd6f-b59c-48ed-b742-ce53537b2c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "281d2e46-8378-4df4-8e64-52affca573f5",
   "metadata": {},
   "source": [
    "### some draft/playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465be3c3-67b2-4f80-9775-5781d66ab6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.query('month == \"mar\"')['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221b9c4d-49b3-42b0-850d-f940ddc694bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.query('month == \"mar\"')['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a21fe0-5557-4d26-a9a9-1ee61057ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.query('month == \"apr\"')['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4896e60e-18b6-4caa-9310-d77a61e34dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.query('month == \"apr\"')['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c163fa1-2e2a-4287-9839-fb859ea376d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importance.query(\"~feature.str.startswith('month') & ~feature.str.startswith('day')\").sort_values('coef', ascending=False).style.format(\n",
    "    precision=3\n",
    ").background_gradient(\n",
    "    cmap=\"PiYG\",\n",
    "    vmin=-2,\n",
    "    vmax=2,\n",
    "    axis=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc950f9-1d68-448a-b4ac-3f261ad1422a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:522]",
   "language": "python",
   "name": "conda-env-522-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
